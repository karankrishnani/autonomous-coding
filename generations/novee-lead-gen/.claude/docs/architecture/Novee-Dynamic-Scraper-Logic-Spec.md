# ğŸ“„ Scraper Logic Configuration Spec â€” Novee MVP

> Enable patchable and maintainable scraping logic for platforms like Slack, LinkedIn, and Reddit by decoupling extraction logic from hardcoded selectors.

---

## ğŸ§  Overview

This spec outlines the architecture and implementation for a **remotely updateable scraper system** in the Novee desktop app. The goal is to allow scraping rules (DOM selectors, filters, transformations) to be updated without requiring users to download a new app version.

---

## ğŸ§± Architecture Goals

- âœ… Decouple scraping logic (selectors, filters) from code
- âœ… Support hot patching of broken scrapers
- âœ… Allow remote config delivery via Supabase or Vercel
- âœ… Enable platform-specific logic via pluggable modules

---

## ğŸ—‚ï¸ Folder Structure (Example)

```
scrapers/
  â”œâ”€ SlackScraper.ts
  â”œâ”€ LinkedInScraper.ts
  â”œâ”€ RedditScraper.ts
  â””â”€ configLoader.ts
```

---

## ğŸ”Œ Scraper Module Interface

Each scraper is a standalone module that accepts a config object and returns extracted leads in a consistent format.

```ts
// SlackScraper.ts

type ScraperConfig = {
  containerSelector: string;
  postSelector: string;
  timeSelector?: string;
  filters: {
    keywords: string[];
    excludeUsers?: string[];
  };
};

export const SlackScraper = async (config: ScraperConfig) => {
  const posts = await scrapeDOM({
    containerSelector: config.containerSelector,
    postSelector: config.postSelector,
    timeSelector: config.timeSelector,
    filters: config.filters,
  });

  return posts.map(post => ({
    message: post.text,
    timestamp: post.time,
    source_url: post.link,
  }));
};
```

---

## ğŸŒ Remote Config Fetching

Scraper configs are pulled from the backend via HTTPS on app launch and optionally on a regular interval or on scraping failure.

```ts
// configLoader.ts

export const fetchScraperConfig = async (platform: 'slack' | 'linkedin' | 'reddit') => {
  const res = await fetch(`https://yourbackend.com/scraper-config/${platform}`);
  return await res.json(); // Expected to match ScraperConfig type
};
```

---

## ğŸ’¾ Supabase DB Schema

```sql
CREATE TABLE scraper_configs (
  platform TEXT PRIMARY KEY,
  version TEXT,
  config JSONB,
  updated_at TIMESTAMP DEFAULT NOW()
);
```

---

## ğŸ”„ JSON Config Example

```json
{
  "containerSelector": ".p-workspace__primary_view",
  "postSelector": ".c-message__content",
  "timeSelector": "time",
  "filters": {
    "keywords": ["hiring", "freelancer", "contract", "agency"],
    "excludeUsers": ["bot", "recruiter"]
  }
}
```

---

## âš ï¸ Fallback + Error Strategy

- If config fetch fails, use last-known-good local config file
- On scrape failure:
  - Retry with freshly fetched config
  - If still broken, log + report to backend
- Optionally show a â€œScraper update in progressâ€ toast to user

---

## ğŸ§ª DevTool Ideas (Optional)

Build a hidden admin/dev panel that:
- Loads current DOM
- Allows testing new selectors live
- Preview output post-transformation
- Sends tested config to backend for promotion

---

## âœ… Benefits

- React quickly to DOM or layout changes on Slack/LinkedIn
- Centralized control of all scraper logic
- Reduced app rebuilds and user update churn
- Unlocks A/B testing of scraper filters in the future

---

## ğŸ”® Future Extensions

- Versioned configs with rollback support
- Platform-specific scraping agents dispatched dynamically
- Store logs of config performance (posts matched, error rate)
